# -*- coding: utf-8 -*-
"""GAT .ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-MZ-qRixcr5e-TnLY7L07grnl53EpjQj
"""

!pip install dgl

from dgl.nn.pytorch import GATConv

import torch
import torch.nn as nn
import torch.nn.functional as F

class GATLayer(nn.Module):
  def __init__(self, g ,in_dim,out_dim):
    super(GATLayer, self).__init__()
    self.g = g
    #equation (1)
    self.fc = nn.Linear(in_dim, out_dim, bias = False)
    #equation (2)
    self.attn_fc = nn.Linear(2*out_dim, 1, bias=False)
    self.reset_parameters()

  def reset_parameters(self):
    """Reinitialize learnable parameters."""
    gain = nn.init.calculate_gain('relu')
    nn.init.xavier_normal_(self.fc.weightm gain = gain)
    nn.init.xavier_normal_(self.attn_fc.weightm gain = gain )

  def edge_attention(self, edges):
    # edge UDF for equation (2)
    z2 =

!pip install pytorch_lightning

!pip install torchvision

!pip install torch torchvision torchaudio

# import libraries
import os
import math
import numpy as np
import time

# for plotting
import matplotlib.pyplot as plt
import seaborn as sns
import tqdm

# pytorch

import torch
import torch.nn as nn
import torch.functional as F
import torch.utils.data as data
import torch.optim as optim

# torchvision
import torchvision
from torchvision.datasets import CIFAR10
from torchvision import transforms

#import pytorch_lightning as pl
#from pytorch_lightning.callbacks import LearningRateMonitor, ModelCheckpoint

"""#CORA DATASET"""

!pip install torch_geometric
!pip install spektral

!pip show spektral
!pip install --upgrade spektral
!pip install keras-gcn

import torch
import torch.nn.functional as F
import time
import numpy as np
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GATConv
from torch_geometric.data import DataLoader
from spektral.datasets import citation
#from spektral.layers import GraphConv
#from spektral.utils.convolution import localpooling_filter
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dropout, Dense
from tensorflow.keras import Sequential
from tensorflow.keras.optimizers import Adam

# Load the Cora dataset
!pip install torch_geometric
!pip show spektral
!pip install --upgrade spektral
!pip install keras-gcn
import torch
import torch.nn.functional as F
import time
import numpy as np
from torch_geometric.datasets import Planetoid
from torch_geometric.nn import GATConv
from torch_geometric.data import DataLoader
from spektral.datasets import citation
#from spektral.layers import GraphConv
#from spektral.utils.convolution import localpooling_filter
from tensorflow.keras.models import Model
from tensorflow.keras.layers import Input, Dropout, Dense
from tensorflow.keras import Sequential
from tensorflow.keras.optimizers import Adam
from torch_geometric.datasets import Planetoid
dataset = Planetoid(root='/Planetoid', name='Cora')
data = dataset[0]

dataset

!pip install matplotlib
import matplotlib.pyplot as plt
import numpy as np
import time  # Import the time module
class GATNet(torch.nn.Module):
    def __init__(self):
        super(GATNet, self).__init__()
        self.conv1 = GATConv(dataset.num_node_features, 4, heads=4, dropout=0.6)
        self.conv2 = GATConv(4 *4 , dataset.num_classes, heads=1, concat=True, dropout=0.6)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv1(x, edge_index)
        x = F.relu(x)
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Initialize the model and optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GATNet().to(device)
data = data.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)

# Function to evaluate the model
def evaluate(model, data, mask):
    model.eval()
    with torch.no_grad():
        out = model(data)
        loss = F.nll_loss(out[mask], data.y[mask])
        _, pred = out[mask].max(dim=1)
        correct = float(pred.eq(data.y[mask]).sum().item())
        acc = correct / mask.sum().item()
    return loss.item(), acc
# Early stopping setup
best_val_loss = float('inf')
patience = 3
counter = 0


# Initialize empty lists to store training and validation loss and accuracy
epochs =[]
train_losses = []
val_losses = []
train_accuracies = []
val_accuracies = []
test_losses = []
test_accuracies = []

# Initialize an array to store the x-axis values for plotting
epoch_values = []  # This will store the epoch numbers where you have recorded validation losses


start_time = time.time()  # Record the start time
# Training the model
model.train()
for epoch in range(100):  # Keep training for a fixed number of epochs (100) without early stopping
    optimizer.zero_grad()
    out = model(data)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

    # Evaluate the model
    train_loss, train_acc = evaluate(model, data, data.train_mask)
    val_loss, val_acc = evaluate(model, data, data.val_mask)
    test_loss, test_acc = evaluate(model, data, data.test_mask)


    # Append the current epoch to the epoch_values list
    epochs.append(epoch + 1)

    # Append the values to the respective lists
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)




    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}')
    print(f'Train Accuracy: {train_acc:.4f}, Val Accuracy: {val_acc:.4f}, Test Accuracy: {test_acc:.4f}')

    # Check for early stopping
    #if val_loss < best_val_loss:
       # best_val_loss = val_loss
       # counter = 0
   # else:
       # counter += 1
       # if counter >= patience:
         #   print("Early stopping")
          #  break  # Break the loop only when early stopping criteria are met
end_time = time.time()  # Record the end time
training_time = end_time - start_time
print(f"Training time: {training_time:.2f} seconds")

model.eval()
_, pred = model(data).max(dim=1)
correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())
acc = correct / data.test_mask.sum().item()
print(f'Accuracy: {acc*100:.4f}')

# Create an array of epoch numbers for x-axis
epochs = np.arange(1, 101)  # Assuming 200 epochs

print(len(epochs))
print(len(val_losses))
print(len(test_losses))
print(epochs)
print(val_losses)
print(test_losses)
# Plotting
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, val_losses, label='Validation Loss')
plt.plot(epochs, test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Validation and Test Loss Over Epochs')
plt.show()
print("-------------------------------------------------------")
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, val_accuracies, label='Validation accuracy')
plt.plot(epochs, test_accuracies, label='Test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Validation and Test Accuracy Over Epochs')
plt.show()



# Load the Cora dataset
dataset = Planetoid(root='/Planetoid', name='PubMed')
data = dataset[0]

dataset

class GATNet(torch.nn.Module):
    def __init__(self):
        super(GATNet, self).__init__()
        self.conv1 = GATConv(dataset.num_node_features, 8, heads=8, dropout=0.6)
        self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=True, dropout=0.3)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = F.dropout(x, p=0.3, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Initialize the model and optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GATNet().to(device)
data = data.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)

# Training the model
model.train()
for epoch in range(300):
    optimizer.zero_grad()
    out = model(data)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()
    print(f'Epoch {epoch+1}, Loss: {loss.item()}')

# Testing the model
model.eval()
_, pred = model(data).max(dim=1)
correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())
acc = correct / data.test_mask.sum().item()
print(f'Accuracy: {acc:.4f}')

# Load the Cora dataset
dataset = Planetoid(root='/Planetoid', name='Citeseer')
data = dataset[0]

dataset

import matplotlib.pyplot as plt
import numpy as np

class GATNet(torch.nn.Module):
    def __init__(self):
        super(GATNet, self).__init__()
        self.conv1 = GATConv(dataset.num_node_features, 8, heads=8, dropout=0.6)
        self.conv2 = GATConv(8 * 8, dataset.num_classes, heads=1, concat=True, dropout=0.3)

    def forward(self, data):
        x, edge_index = data.x, data.edge_index
        x = F.dropout(x, p=0.6, training=self.training)
        x = self.conv1(x, edge_index)
        x = F.elu(x)
        x = F.dropout(x, p=0.3, training=self.training)
        x = self.conv2(x, edge_index)
        return F.log_softmax(x, dim=1)

# Initialize the model and optimizer
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = GATNet().to(device)
data = data.to(device)
optimizer = torch.optim.Adam(model.parameters(), lr=0.001, weight_decay=5e-4)

# Function to evaluate the model
def evaluate(model, data, mask):
    model.eval()
    with torch.no_grad():
        out = model(data)
        loss = F.nll_loss(out[mask], data.y[mask])
        _, pred = out[mask].max(dim=1)
        correct = float(pred.eq(data.y[mask]).sum().item())
        acc = correct / mask.sum().item()
    return loss.item(), acc


# Initialize empty lists to store training and validation loss and accuracy
train_losses = []
val_losses = []
train_accuracies = []
val_accuracies = []
test_losses = []
test_accuracies = []

# Training the model
model.train()
for epoch in range(200):
    optimizer.zero_grad()
    out = model(data)
    loss = F.nll_loss(out[data.train_mask], data.y[data.train_mask])
    loss.backward()
    optimizer.step()

    # Evaluate the model
    train_loss, train_acc = evaluate(model, data, data.train_mask)
    val_loss, val_acc = evaluate(model, data, data.val_mask)
    test_loss, test_acc = evaluate(model, data, data.test_mask)

    # Append the values to the respective lists
    train_losses.append(train_loss)
    val_losses.append(val_loss)
    train_accuracies.append(train_acc)
    val_accuracies.append(val_acc)
    test_losses.append(test_loss)
    test_accuracies.append(test_acc)

    print(f'Epoch {epoch+1}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}, Test Loss: {test_loss:.4f}')
    print(f'Train Accuracy: {train_acc:.4f}, Val Accuracy: {val_acc:.4f}, Test Accuracy: {test_acc:.4f}')
model.eval()
_, pred = model(data).max(dim=1)
correct = float(pred[data.test_mask].eq(data.y[data.test_mask]).sum().item())
acc = correct / data.test_mask.sum().item()
print(f'Accuracy: {acc:.4f}')

# Create an array of epoch numbers for x-axis
epochs = np.arange(1, 201)  # Assuming 200 epochs

print(len(epochs))
print(len(val_losses))
print(len(test_losses))
print(epochs)
print(val_losses)
print(test_losses)
# Plotting
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, val_losses, label='Validation Loss')
plt.plot(epochs, test_losses, label='Test Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend()
plt.title('Validation and Test Loss Over Epochs')
plt.show()
print("-------------------------------------------------------")
plt.figure(figsize=(12, 5))
plt.subplot(1, 2, 1)
plt.plot(epochs, val_accuracies, label='Validation accuracy')
plt.plot(epochs, test_accuracies, label='Test accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend()
plt.title('Validation and Test Accuracy Over Epochs')
plt.show()

# Commented out IPython magic to ensure Python compatibility.
# %%capture
# import os
# import torch
# os.environ['TORCH'] = torch.__version__
# os.environ['PYTHONWARNINGS'] = "ignore"
# !pip install torch-scatter -f https://data.pyg.org/whl/torch-${TORCH}.html
# !pip install torch-sparse -f https://data.pyg.org/whl/torch-${TORCH}.html
# !pip install git+https://github.com/pyg-team/pytorch_geometric.git

from torch_geometric.datasets import Planetoid
from torch_geometric.transforms import NormalizeFeatures

dataset = Planetoid(root='data/Planetoid', name='Cora', transform=NormalizeFeatures())

print(f'Dataset: {dataset}:')
print('---------------------------------')
print(f'Number of graphs: {len(dataset)}')
print(f'Number of features: {dataset.num_features}')
print(f'Number of classes: {dataset.num_classes}')

data=dataset[0]

print(data)