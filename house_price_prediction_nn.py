# -*- coding: utf-8 -*-
"""House-price-prediction-NN.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MLXqtDWU1rZssMysjTbAvay2AMQ2hoot

#HOUSE_PRICE_PREDICTION_WITH_NEURAL_NETWORK
features

    Year of sale of the house
    The age of the house at the time of sale
    Distance from city center
    Number of stores in the locality
    The latitude
    The longitude
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd
import matplotlib.pyplot as plt
import tensorflow as tf
import seaborn as sns

#from utils import *
from sklearn.model_selection import train_test_split
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Dropout
from tensorflow.keras.callbacks import EarlyStopping, LambdaCallback

# %matplotlib inline
tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)

print('Libraries are imported.')

"""#DATASET"""

import pandas as pd


# Create a dictionary with column names as keys and new values as values
#new_row = {
 #   'serial'
#}

# Use the loc indexer to insert the new row
#data = data.append(new_row, ignore_index=True)

# Display the DataFrame with the new row

data = pd.read_csv('Data_Set.csv' )
# List of variable names
#variable_names = ['serial', 'date', 'age', 'distance', 'stores', 'latitude', 'longitude', 'price']

# Add the variable names as the first row
#data.loc[-1] = variable_names
#d#ata.index = data.index + 1  # Shift index to make room for the new row
#data.sort_index(inplace=True)  # Sort the DataFrame by index


data.head()

data.info()

data.describe()

data.isna().sum() # check for missing values

data.shape

print(data.dtypes)

sns.distplot(data['  price']) # to see the price as gausian plot

data.hist(figsize=(15,8) )

data["  price"]

plt.figure(figsize=(10,6))
sns.heatmap(data.corr(),
            cmap = 'BrBG',
            fmt = '.2f',
            linewidths =2,
            annot = True)

"""#Checking for missing Data and outliers"""

data.isnull().sum() # isna() gives true or false value (1/0) if the data in the column is missing or present respectively
                # sum() then counts the total missing fields in all columns

data.isna().sum()

"""#DATA NORMALIZATION
Data is normalized so as to bring all the different features to a similar range, thus to make it easier for optimization algorithms to find minimas.
"""

data = data.iloc[:,1:]
data_norm = (data - data.mean()) / data.std() #Data is normalized by subtracting each value in the column with the mean value and then dividing it with the standard                                                 deviation of the whole column
data_norm.head()

"""#

Convert Label Value Back To Original:

The Labelvalues are normalized, thus we'll get predictions from the trained model in the same range and are required to be original range.

"""

y_mean = data['  price'].mean()
y_std = data['  price'].std()

def convert_label_value(pred):       #Defining a function which will convert the label values back to the original distribution and return it
    return int(pred * y_std + y_mean)
y_mean

"""
#Creating Training and Test Sets



Train and Test Split:

We will keep some part of the data aside as a test set. The model will not use this set during training and it will be used only for checking the performance of the model in trained and un-trained states.


"""

x = data_norm.iloc[:, :6] # storing the features ecxcept the 6th feature
x.head()

y = data_norm.iloc[:, -1]
y.head()

y.shape

x.shape

x_train, x_test , y_train, y_test = train_test_split(x,y,test_size = 0.05, shuffle = True, random_state=0)

#This predefined function splits the dataset to train and test set, where test size is given in 'test_size'(Here 5%)
#Random state ensures that the splits that you generate are reproducible. Scikit-learn uses random permutations to generate the splits.

print('x_train shape: ', x_train.shape)
print('y_train shape: ', y_train.shape)
print('x_test shape: ', x_test.shape)
print('y_test shape: ', y_test.shape)

"""#Creating the Model


Defining the Model:

Here a function is defined that returns an untrained neural network model of a certain architecture. Here I've created a model with 2 hidden layers.

Input layer consists of 10 neurons (10(6+1) = 70 weights/parameters), hidden layers consists of 20 (20(10+1)=220 weights/parameters) and 5 (5*(20+1)=105 weights/parameters) neurons respectively (Extra 1 Input to neurons is the biased unit).

A neural network without an activation function is essentially just a linear regression model. The activation function does the non-linear transformation to the input making it capable to learn and perform more complex tasks. Since it is a regression problem 'relu' (Rectified linear unit) is used.

"""

import numpy as np
import pandas as pd
import tensorflow as tf
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score
from sklearn.metrics import mean_absolute_percentage_error
from sklearn.metrics import mean_absolute_percentage_error, r2_score
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

model = tf.keras.Sequential([
  tf.keras.layers.Dense(64, activation='relu', input_shape=(x_train.shape[1],)),
  tf.keras.layers.Dense(32, activation='softmax'),
  tf.keras.layers.Dense(1)
])

model.compile(
        loss = 'mse',
        optimizer ='adam'
    )
     # Train the model
model.fit(x_train,y_train, epochs=10, batch_size=16, validation_split=0.2)

    # Evaluate the model on the test set
y_pred = model.predict(x_test)
mape = mean_absolute_percentage_error(y_test, y_pred)

print(f"Mean Absolute Percentage Error on test set: {mape:.4f}")


r2 = r2_score(y_test, y_pred)
print(f"R-squared: {r2:.4f}")

# Calculate Mean Absolute Error (MAE)
mae = mean_absolute_error(y_test, y_pred)

# Calculate Mean Squared Error (MSE)
mse = mean_squared_error(y_test, y_pred)

# Calculate Root Mean Squared Error (RMSE)
rmse = np.sqrt(mse)

# Calculate R-squared (R2)
r2 = r2_score(y_test, y_pred)

# Print the metrics
print(f"Mean Absolute Error (MAE): {mae:.4f}")
print(f"Mean Squared Error (MSE): {mse:.4f}")
print(f"Root Mean Squared Error (RMSE): {rmse:.4f}")
print(f"R-squared (R2): {r2:.4f}")

"""
#Model Training


EarlyStopping callback is used from Keras to stop the model training if the validation loss stops decreasing for a few epochs (Here, after 5 epochs).
"""

early_stopping = EarlyStopping(monitor='val_loss', patience = 5) #Defining early stopping parameter (optional, to save time)

model = the_model()

preds_on_untrained = model.predict(x_test) #Make predictions on the test set before training the parameters

#Finally training the model-->
history = model.fit(
    x_train, y_train,
    validation_data = (x_test, y_test),
    epochs = 100,
    callbacks = [early_stopping]
)

